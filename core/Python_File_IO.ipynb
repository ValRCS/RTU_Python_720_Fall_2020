{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l4QTcjrghYo2"
   },
   "source": [
    "##  File I/O (Input / Output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KSgkKXwfhYo6"
   },
   "source": [
    "## Jupyter only writing to text file\n",
    "* %%writefile filename.ext\n",
    "\n",
    "Writes in the working directory (first run pwd) \n",
    "\n",
    "This is not Python specific command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Github\\\\RTU_Python_720_Fall_2020\\\\core'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouKJsbyGhYo8",
    "outputId": "dc2bb26b-9558-4813-c30a-d56725793423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mylib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mylib.py\n",
    "# this is a small .py file that we will use for as a module(import)\n",
    "import math # importing standard Python math library\n",
    "MY_PI = 3.1415926\n",
    "\n",
    "def nb_year(pop_start, percent, yearly_arrival, pop_end):\n",
    "    count = 0\n",
    "    population = pop_start\n",
    "    while population < pop_end:\n",
    "        # short hand population *= (1+percent/100)\n",
    "        # also shortone population += population * percent / 100\n",
    "        population = population + math.floor(population * percent / 100)\n",
    "        population += yearly_arrival\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "# could add main guard here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VPuZwNoRhYo9"
   },
   "outputs": [],
   "source": [
    "import mylib # it works because C:\\PyLib\\mylib.py is in my PYTHONPATH enviroment variables\n",
    "# import looks FIRST in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SnvHsFrne0e",
    "outputId": "2b1a97e5-cc23-4686-ec41-0f5f626e130a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415926\n"
     ]
    }
   ],
   "source": [
    "print(mylib.MY_PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXhcK3eghYo9",
    "outputId": "77b662ee-bc19-4d8b-e047-97dbdfd8a7eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylib.nb_year(100,2,0,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2KNz6Uwnz7K",
    "outputId": "e3f6a1b8-0cf9-4838-9891-be7a8d352205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylib.add(5,15)  # so adding on the fly is problematic on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg5f7Ux-_nYq"
   },
   "source": [
    "![Frost](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Robert_Frost_NYWTS_3.jpg/440px-Robert_Frost_NYWTS_3.jpg)\n",
    "\n",
    "[Robert Frost](https://en.wikipedia.org/wiki/Robert_Frost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiDWH112hYo7",
    "outputId": "742ddfd5-0604-46b6-ed94-cd55d8298c42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing two_roads.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile two_roads.txt\n",
    "Robert Frost\n",
    "The Road not Taken\n",
    "\n",
    "Two roads diverged in a yellow wood,\n",
    "And sorry I could not travel both\n",
    "And be one traveler, long I stood\n",
    "And looked down one as far as I could\n",
    "To where it bent in the undergrowth;\n",
    "\n",
    "Then took the other, just as fair,\n",
    "And having perhaps the better claim,\n",
    "Because it was grassy and wanted wear;\n",
    "Though as for that the passing there\n",
    "Had worn them really about the same,\n",
    "\n",
    "3rd verse\n",
    "\n",
    "And both that morning equally lay\n",
    "In leaves, no step had trodden black.\n",
    "Oh, I kept the first for another day!\n",
    "Yet knowing how way leads on to way,\n",
    "I doubted if I should ever come back.\n",
    "\n",
    "4th verse\n",
    "\n",
    "I shall be telling this with a sigh\n",
    "Somewhere ages and ages hence:\n",
    "Two roads diverged in a wood, and I—\n",
    "I took the one less traveled by,\n",
    "And that has made all the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQftoKXcAN5K"
   },
   "source": [
    "## Path from pathlib library\n",
    "\n",
    "pathlib is part of standart Python library - comes with every Python since I believe 3.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNiXxb-CoXR5"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path # pathlib is the modern way to handle file paths in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is: d:\\Github\\RTU_Python_720_Fall_2020\\core\n"
     ]
    }
   ],
   "source": [
    "current_work_directory = Path.cwd()\n",
    "# this will give us absolute path to the current working directory\n",
    "print(f\"Current working directory is: {current_work_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding what we have in current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owFkKdEWobKo",
    "outputId": "380d8746-8df4-45fb-b5e5-a5d23ee0a6a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('alice_queen.txt'),\n",
       " WindowsPath('requirements.txt'),\n",
       " WindowsPath('somefile.txt'),\n",
       " WindowsPath('two_roads.txt')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are using list comprehension to get all files in the current directory\n",
    "# we are using . to represent the current directory\n",
    "# we are using * to represent all files\n",
    "# here in the glob - which is for current directory - we are looking for all .txt files\n",
    "# we obtain a list of Path objects as a result\n",
    "files = [f for f in Path(\".\").glob(\"*.txt\") if f.is_file()]\n",
    "files # note it gives Windows path on Windows OS\n",
    "# on Mac we would see Posix path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poaNKG7WzZFp",
    "outputId": "748f1d4f-06cb-40bc-b072-537eae8db538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last file in the list is: two_roads.txt\n",
      "Name of the file is: two_roads.txt\n",
      "Stem of the file is: two_roads\n",
      "Extension of the file is: .txt\n"
     ]
    }
   ],
   "source": [
    "# let's get the last file in the list\n",
    "file_path = files[-1]\n",
    "print(f\"Last file in the list is: {file_path}\") # notice when we print this Windows Path\n",
    "# we are only shown the string representation of the Path object\n",
    "# we can get the name of the file by using the .name attribute\n",
    "print(f\"Name of the file is: {file_path.name}\")\n",
    "# we can get stem of the file by using the .stem attribute\n",
    "print(f\"Stem of the file is: {file_path.stem}\")\n",
    "# we can get extension of the file by using the .suffix attribute\n",
    "print(f\"Extension of the file is: {file_path.suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwjXboTHzgAd",
    "outputId": "78b4a93c-c87a-495c-bf9e-72a26e0c518a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the file: two_roads.txt\n",
      "Got 783 symbols in two_roads.txt\n",
      "Got 0 symbols in another_text_read\n",
      "Got 783 symbols in yet_another_text_read\n"
     ]
    }
   ],
   "source": [
    "print(f\"Opening the file: {file_path.name}\")\n",
    "with open(file_path, encoding=\"utf-8\") as f: #note that mode=\"r\" is the default mode\n",
    "    text_from_file = f.read() # generally this is all you need to read the file\n",
    "    # file is still open here\n",
    "    another_text_read = f.read() # this will read nothing because we are at the end of the file\n",
    "    # it is possible to reset the file pointer to the beginning of the file\n",
    "    f.seek(0) # we can start over, we can seek also to specific position in file\n",
    "    # seek might be useful if you have a non standard file format and need to skip some bytes\n",
    "    yet_another_text_read = f.read() # this will read the file again\n",
    "    # again file is still open but again exhausted\n",
    "# file is automatically closed after the block\n",
    "print(\"Got\", len(text_from_file), \"symbols in\", file_path)\n",
    "# now let's see how many symbols we got in another_text_read\n",
    "print(\"Got\", len(another_text_read), \"symbols in another_text_read\")\n",
    "# now let's see how many symbols we got in yet_another_text_read\n",
    "print(\"Got\", len(yet_another_text_read), \"symbols in yet_another_text_read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eYp0mFrk118U",
    "outputId": "67543d57-4eae-4598-f94f-7141696d80de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # this includes some usefule constants like\n",
    "string.punctuation # not all punctuation is here, but most common is\n",
    "# so this could serve as a good start for cleaning up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe to clean up bad characters in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-hAsBze16Nc",
    "outputId": "e0536d30-b098-497b-c652-0649cb91e857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 783 symbols before cleaning\n",
      "Got 765 symbols after cleaning\n"
     ]
    }
   ],
   "source": [
    "clean_text = text_from_file # here text is still dirty...\n",
    "# how many we have before cleaning\n",
    "print(\"Got\", len(clean_text), \"symbols before cleaning\")\n",
    "for punct in string.punctuation:\n",
    "    clean_text = clean_text.replace(punct, \"\") # so i can replace all occurences of all punctuation\n",
    "print(\"Got\", len(clean_text), \"symbols after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Frost\n",
      "The Road not Taken\n",
      "\n",
      "Two roads diverged in a yellow wood\n",
      "And sorry I could not travel both\n",
      "And be one traveler long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth\n",
      "\n",
      "Then took the other just as fair\n",
      "And having perhaps the better claim\n",
      "Because it was grass\n"
     ]
    }
   ],
   "source": [
    "# head of cleaned text\n",
    "# first 300 symbols\n",
    "print(clean_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wl-gYcMvzvj_",
    "outputId": "ed09b62d-5ff7-44c3-98b0-196b8b497915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 153 tokens in two_roads.txt\n"
     ]
    }
   ],
   "source": [
    "# tokens = alice_text.split()\n",
    "tokens = clean_text.split() # very handy because it splits by ANY whitespace, space, tab, newline, and more esoteric ones\n",
    "print(\"Got\", len(tokens), \"tokens in\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9Cipagsz6vN",
    "outputId": "14f1a627-4124-44bc-af05-238fd1c2b35a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 8),\n",
       " ('the', 8),\n",
       " ('And', 6),\n",
       " ('as', 4),\n",
       " ('in', 3),\n",
       " ('a', 3),\n",
       " ('one', 3),\n",
       " ('and', 3),\n",
       " ('that', 3),\n",
       " ('not', 2),\n",
       " ('Two', 2),\n",
       " ('roads', 2),\n",
       " ('diverged', 2),\n",
       " ('wood', 2),\n",
       " ('could', 2),\n",
       " ('both', 2),\n",
       " ('be', 2),\n",
       " ('it', 2),\n",
       " ('took', 2),\n",
       " ('for', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter(tokens)\n",
    "word_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization before counting\n",
    "\n",
    "We can see in the above example that the same word is counted twice because of the different case. We can normalize the text to lowercase before counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 153 tokens in lowercase_tokens\n"
     ]
    }
   ],
   "source": [
    "# we have two ways of fixing it, we could lowercase the original text\n",
    "# or we could lowercase the tokens we just created\n",
    "# in a way it is easier to lowercase the original text and THEN split again\n",
    "# but for now let's see how we can lowercase the tokens\n",
    "# we could use list comprehension\n",
    "# but for this we will use good old for loop\n",
    "lowercase_tokens = []\n",
    "for token in tokens:\n",
    "    lowercase_tokens.append(token.lower())\n",
    "# let's see how many tokens you got\n",
    "print(\"Got\", len(lowercase_tokens), \"tokens in lowercase_tokens\")\n",
    "# we could even make a very basic assertion that length of tokens and lowercase_tokens should be the same\n",
    "assert len(tokens) == len(lowercase_tokens), \"Number of tokens should be the same\"\n",
    "# if assertion is correct, nothing happens, if it is wrong, an exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9),\n",
       " ('and', 9),\n",
       " ('i', 8),\n",
       " ('in', 4),\n",
       " ('as', 4),\n",
       " ('a', 3),\n",
       " ('one', 3),\n",
       " ('that', 3),\n",
       " ('not', 2),\n",
       " ('two', 2),\n",
       " ('roads', 2),\n",
       " ('diverged', 2),\n",
       " ('wood', 2),\n",
       " ('could', 2),\n",
       " ('both', 2),\n",
       " ('be', 2),\n",
       " ('to', 2),\n",
       " ('it', 2),\n",
       " ('took', 2),\n",
       " ('for', 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's count again\n",
    "lower_counter = Counter(lowercase_tokens)\n",
    "lower_counter.most_common(20) # this is much better now our counts are not case sensitive\n",
    "# we see than and And are now counted together\n",
    "# same for the and The\n",
    "# small downside is that we lost the original case of the words such a I and Two\n",
    "# there are cases where case is important(pun intended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "J37XsnqV0In4"
   },
   "outputs": [],
   "source": [
    "# CSV - comma separareted files are just text files with some structure\n",
    "N = 50 # of course could use any other name for the variable\n",
    "with open(\"forst_word_stats.csv\", mode=\"w\", encoding=\"utf-8\") as f: # to write I need to specify mode=\"w\"\n",
    "    f.write(\"word, frequency\\n\") # we write a single row\n",
    "    # f.write(\"\\n\".join(word_count.most_common(N)))\n",
    "    for my_tuple in lower_counter.most_common(N): # it is easier to change values in one place\n",
    "        f.write(f\"{my_tuple[0]},{my_tuple[1]}\\n\") # most common returns a list of tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing file names using Python\n",
    "\n",
    "It is possible to change file names using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest csv file is: forst_word_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# let's find out our latest text file that we created\n",
    "csv_files = [f for f in Path(\".\").glob(\"*.csv\") if f.is_file()]\n",
    "# let's sort by modification time\n",
    "# the lambda function is a way to define a function in one line\n",
    "# sort can take optional argument \n",
    "# key which is a function that will be called on each element before sorting\n",
    "# the effect that we are sorting files not by the file name but by the modification time\n",
    "csv_files.sort(key=lambda f: f.stat().st_mtime)\n",
    "# latest file is the last one\n",
    "latest_csv = csv_files[-1]\n",
    "# let's print the name of the latest file\n",
    "# we could print the modification time as well but it is not very human readable\n",
    "# it is based on epoch time starting from 1970.1.1\n",
    "print(f\"Latest csv file is: {latest_csv.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('frost_word_stats.csv')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so let's rename this file to something more meaningful\n",
    "# the correct name is \"frost_word_stats.csv\"\n",
    "# we could use the .rename method of the Path object\n",
    "# we could also use the .replace method of the string object\n",
    "\n",
    "# let's use rename\n",
    "# latest_csv must be Path object for this to work\n",
    "\n",
    "latest_csv.rename(\"frost_word_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading lines from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 30 lines in two_roads.txt\n"
     ]
    }
   ],
   "source": [
    "# we could read all text lines at once\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()   \n",
    "    # still open but seek is at the end\n",
    "# file is closed here\n",
    "# how many lines we got\n",
    "print(\"Got\", len(lines), \"lines in\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading rows of text of larger files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2SjelltphYpE",
    "outputId": "ae4702f8-1d1e-4f15-9d8d-befaa47f2963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Frost\n",
      "The Road not Taken\n",
      "\n",
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "Then took the other, just as fair,\n",
      "And having perhaps the better claim,\n",
      "Because it was grassy and wanted wear;\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "3rd verse\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves, no step had trodden black.\n",
      "Oh, I kept the first for another day!\n",
      "Yet knowing how way leads on to way,\n",
      "I doubted if I should ever come back.\n",
      "\n",
      "4th verse\n",
      "\n",
      "I shall be telling this with a sigh\n",
      "Somewhere ages and ages hence:\n",
      "Two roads diverged in a wood, and I—\n",
      "I took the one less traveled by,\n",
      "And that has made all the difference.\n"
     ]
    }
   ],
   "source": [
    "# instead of reading the file all at once we can read it line by line\n",
    "# this is beneficial for very large files\n",
    "# we could even process a 1TB file with no problem\n",
    "# as long as we do not try to load it all at once\n",
    "# of course this file should have more than 1 line :)\n",
    "\n",
    "# let's read the file line by line\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for row in f:\n",
    "        print(row, end=\"\")\n",
    "        # we could have done something else here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 19, 1, 37, 34, 34, 38, 37, 1, 35, 37, 39, 37, 37, 1, 10, 1, 34, 38, 38, 37, 38, 1, 10, 1, 36, 31, 37, 33, 38]\n"
     ]
    }
   ],
   "source": [
    "# so let's use this approach to save length of each row\n",
    "row_counts = []\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for row in f:\n",
    "        row_counts.append(len(row)) # so we count the length of each row one by one\n",
    "# let's see what we got\n",
    "print(row_counts)\n",
    "# note that row counts also includes newline characters\n",
    "# so for true characters we should subtract 1 from each count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HH9Z4K0nhYpI"
   },
   "source": [
    "## Use with open always! \n",
    "\n",
    "* closes automatically!\n",
    "* throws exceptions on errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-EzL_e4hYpI",
    "outputId": "a4051f2a-61b6-4bf2-d869-26cef43b136b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Frost\n",
      "\n",
      "The Road not Taken\n",
      "\n",
      "\n",
      "\n",
      "Two roads diverged in a yellow wood,\n",
      "\n",
      "And sorry I could not travel both\n",
      "\n",
      "And be one traveler, long I stood\n",
      "\n",
      "And looked down one as far as I could\n",
      "\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "\n",
      "\n",
      "Then took the other, just as fair,\n",
      "\n",
      "And having perhaps the better claim,\n",
      "\n",
      "Because it was grassy and wanted wear;\n",
      "\n",
      "Though as for that the passing there\n",
      "\n",
      "Had worn them really about the same,\n",
      "\n",
      "\n",
      "\n",
      "3rd verse\n",
      "\n",
      "\n",
      "\n",
      "And both that morning equally lay\n",
      "\n",
      "In leaves, no step had trodden black.\n",
      "\n",
      "Oh, I kept the first for another day!\n",
      "\n",
      "Yet knowing how way leads on to way,\n",
      "\n",
      "I doubted if I should ever come back.\n",
      "\n",
      "\n",
      "\n",
      "4th verse\n",
      "\n",
      "\n",
      "\n",
      "I shall be telling this with a sigh\n",
      "\n",
      "Somewhere ages and ages hence:\n",
      "\n",
      "Two roads diverged in a wood, and I—\n",
      "\n",
      "I took the one less traveled by,\n",
      "\n",
      "And that has made all the difference.\n",
      "file is closed already here\n"
     ]
    }
   ],
   "source": [
    "# Idiom on how to open AND close a file for reading and doing work\n",
    "with open('two_roads.txt') as fin: # fin or f or file_in whatever name makes most sense, f is very common\n",
    "    for line in fin:\n",
    "        print(line)\n",
    "        # do wo with each line here,save into a list or other structure\n",
    "    # we can do more work with file here\n",
    "    # maybe fin.seek(0) to read it again for some reason\n",
    "    # File will be closed once this line ends\n",
    "# File is closed now    \n",
    "print(\"file is closed already here\")\n",
    "#closes here!\n",
    "#closes automatically!!! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lBVt7GI8hYpI"
   },
   "source": [
    "## For MacOS and Linux\n",
    "* use pwd to see where you are\n",
    "### myfile = open(\"/Users/MyUserName/SomeFolder/MaybeAnotherFolder/myfile.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykwrj10WhYpJ"
   },
   "source": [
    "## For Windows\n",
    "* use pwd to see where you are\n",
    "### myfile = open(\"C:\\\\Users\\\\MyUserName\\\\SomeFolder\\\\MaybeAnotherFolder\\\\myfile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_GMBNYIhYpJ",
    "outputId": "f4f81984-c74b-4f9f-dc10-b78a259e7368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is 72A3-8E69\n",
      "\n",
      " Directory of d:\\Github\\RTU_Python_720_Fall_2020\\core\n",
      "\n",
      "11/21/2024  05:19 PM    <DIR>          .\n",
      "11/21/2024  05:19 PM    <DIR>          ..\n",
      "02/27/2023  06:11 PM             8,779 alice_queen.txt\n",
      "11/21/2024  05:08 PM               434 frost_word_stats.csv\n",
      "02/27/2023  06:11 PM             8,015 Jupyter Tips.ipynb\n",
      "11/14/2024  04:58 PM             1,064 myAprilMod.py\n",
      "11/14/2024  05:13 PM    <DIR>          myAprilPackage\n",
      "11/21/2024  04:43 PM               611 mylib.py\n",
      "02/27/2023  06:11 PM             1,131 MyMod.ipynb\n",
      "02/27/2023  06:11 PM             1,378 Practice_1.ipynb\n",
      "02/27/2023  06:11 PM            44,701 Python Classes.ipynb\n",
      "04/17/2023  05:44 PM           157,878 Python Dictionaries.ipynb\n",
      "02/27/2023  06:11 PM            72,412 Python File IO.ipynb\n",
      "02/27/2023  06:11 PM            35,579 Python File Operations 2 Binary Files and Pickle.ipynb\n",
      "11/07/2024  04:44 PM            78,151 Python Flow Control.ipynb\n",
      "02/27/2023  06:11 PM               273 Python Flow Control.md\n",
      "04/17/2023  05:44 PM            92,037 Python Functions.ipynb\n",
      "02/27/2023  06:11 PM               770 Python Functions.md\n",
      "02/27/2023  06:11 PM            55,431 Python Lists.ipynb\n",
      "02/27/2023  06:11 PM               786 Python Reading Writing Files.md\n",
      "04/17/2023  05:44 PM            47,757 Python Sets.ipynb\n",
      "04/17/2023  06:38 PM            39,988 Python Standard Library.ipynb\n",
      "11/07/2024  04:44 PM           122,661 Python Strings.ipynb\n",
      "02/27/2023  06:11 PM            66,652 Python Variables and Data Types.ipynb\n",
      "11/07/2024  04:44 PM            43,227 Python_Classes.ipynb\n",
      "11/07/2024  04:44 PM           171,036 Python_Dictionaries.ipynb\n",
      "05/15/2023  05:15 PM           120,049 Python_File_IO.ipynb\n",
      "02/27/2023  06:11 PM           115,515 Python_Flow_Control.ipynb\n",
      "11/07/2024  04:44 PM           101,705 Python_Functions.ipynb\n",
      "11/07/2024  04:44 PM           176,889 Python_Lists_and_Tuples.ipynb\n",
      "11/07/2024  04:44 PM            48,754 Python_Loops.ipynb\n",
      "11/14/2024  05:50 PM            28,811 Python_Modules_Packages_External_Libraries.ipynb\n",
      "11/07/2024  04:44 PM            41,116 Python_Sets.ipynb\n",
      "11/07/2024  05:49 PM            46,423 Python_Standard_Library.ipynb\n",
      "02/27/2023  06:11 PM           134,136 Python_Strings.ipynb\n",
      "02/27/2023  06:11 PM            73,205 Python_Variables_and_Data_Types.ipynb\n",
      "11/14/2024  05:40 PM               525 requirements.txt\n",
      "02/27/2023  06:11 PM             2,744 RunOtherNotebooks.ipynb\n",
      "11/07/2024  05:26 PM    <DIR>          sample_data\n",
      "02/27/2023  06:11 PM                69 somefile.txt\n",
      "02/27/2023  06:11 PM             8,880 Tuples.ipynb\n",
      "02/27/2023  06:11 PM             2,299 tuples.py\n",
      "11/21/2024  04:44 PM               815 two_roads.txt\n",
      "11/21/2024  04:43 PM    <DIR>          __pycache__\n",
      "              39 File(s)      1,952,686 bytes\n",
      "               5 Dir(s)  15,934,205,952 bytes free\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Magic !someOScommand for example !dir or !ls\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b_5CxrkNhYpJ",
    "outputId": "f10329e4-1203-4d2a-ce09-6c9b27d46a29"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RNd7-qJjhYpJ"
   },
   "outputs": [],
   "source": [
    "# importing OS specific library for system work\n",
    "# idea being that we can do same on Windows/Mac/Linux and not worry about the OS\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AR8W2IbRhYpK"
   },
   "source": [
    "# absolute paths and relative paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering file and writing a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Ob1rQWhOQiCM"
   },
   "outputs": [],
   "source": [
    "# so let's open two files at once\n",
    "# this way we can process the file sequentially\n",
    "# even truly large files can be processed this way\n",
    "\n",
    "# we will use the with statement\n",
    "# we will use the open function\n",
    "with open(file_path, encoding=\"utf-8\") as fin: # so fin is file stream in\n",
    "    with open(\"clean_frost.txt\", mode=\"w\", encoding=\"utf-8\") as fout: # and file stream out\n",
    "        for line in fin:\n",
    "            # here we have a single line from incoming file read into line from fin stream\n",
    "            # we could check if it has some specific properties\n",
    "            # simple one would be whether it has any nonwhitespace characters\n",
    "            if not line.strip(): # means we got nothing after stripping whitespace\n",
    "                # we do nothing with empty lines and continue to the next lin\n",
    "                continue\n",
    "\n",
    "            # let's keep all lines that start with And\n",
    "            if line.startswith(\"And\"):\n",
    "                # we could do some processing here\n",
    "                # and write to fout\n",
    "                fout.write(line) # so we are copying the line into the output file\n",
    "\n",
    "\n",
    "# if you know Linux shell command you could have used grep, sed, awk to do the same\n",
    "# but Python approach is more portable and more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzRSQX23hYpN"
   },
   "outputs": [],
   "source": [
    "def get_filtered_list(fname, good_words=()):\n",
    "    lines_to_keep = []\n",
    "    with open(afile, encoding=\"utf-8\") as f:\n",
    "        for line in f: # we go through file line by line\n",
    "            if any(word in line for word in good_words):\n",
    "                lines_to_keep.append(line)\n",
    "    # file is closed here already\n",
    "    print(len(lines_to_keep), f\"lines with {good_words}\")\n",
    "    return lines_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending to a file\n",
    "\n",
    "You can only append data to a file at the end of the file. You can't insert data in the middle of a file or at the beginning of a file.\n",
    "\n",
    "If you need to insert something in middle or beginning, you need to read the file, modify it and write it back completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add a row of stars to clean_frost.txt\n",
    "# also let's add current date and time\n",
    "from datetime import datetime\n",
    "with open(\"clean_frost.txt\", mode=\"a\", encoding=\"utf-8\") as fout: # mode=\"a\" is for append\n",
    "    # a will add to the end of the file\n",
    "    fout.write(\"*\" * 80 + \"\\n\")\n",
    "    fout.write(f\"File clean_frost.txt was created on {datetime.now()}\\n\")\n",
    "    fout.write(\"*\" * 80 + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M-iOFgqihYpT"
   },
   "source": [
    "### Modes:\n",
    "  *  mode='r' - Read Only\n",
    "  * 'w' - Write Only (and will overwrite existing files!!!)\n",
    "  * 'a' - Apend Only (stream is at the end of file!)\n",
    "  * 'r+' - Read and Write\n",
    "  * 'w+' - Write and Read with Overwriting existing/make new files\n",
    "  \n",
    "  From C (fopen)\n",
    "   * ``r+''  Open for reading and writing.  The stream is positioned at the\n",
    "         beginning of the file.\n",
    "         \n",
    "    *   ``w+''  Open for reading and writing.  The file is created if it does not\n",
    "         exist, otherwise it is truncated(**destroyed!**).  The stream is positioned at\n",
    "         the beginning of the file.    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Python File IO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
